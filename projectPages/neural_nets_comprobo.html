<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Person Detection</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body>
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="inner">

							<!-- Logo -->
								<a href="../index.html" class="logo">
									<span class="title">Charlie Weiss</span>
								</a>

							<!-- Nav
								<nav>
									<ul>
										<li><a href="#menu">Menu</a></li>
									</ul>
								</nav>-->

						</div>
					</header>

				<!-- Menu 
					<nav id="menu">
						<h2>Menu</h2>
						<ul>
							<li><a href="../index.html">Home</a></li>
							<li><a href="../engineering+design.html">Engineering+Design Work</a></li>
							<li><a href="../personalprojects.html">Personal Projects</a></li>
							<li><a href="../yarncraft.html">Yarncraft</a></li>
							<li><a href="../elements.html">Elements</a></li>
						</ul>
					</nav>-->

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<h1>Person Detection and Following with Convolutional Neural Nets</h1>
							<p>in collaboration with Matthew Beaudouin-Lafon.</p>
							<h2><a href=https://github.com/charlievweiss/robot_learning>Link to Github</a></h2>
                            <p>Below is the written overview of this project, where Matt and I Implemented a convolutional neural network on a Neato robot using ROS, python, Google Colab notebooks, and Apple's ARKit AR platform on an iPhone. We used camera and position data to train the neural net to predict a person's ground truth position based on their location in the camera frame. You can see the documentation again in the README in our github repository, as well as all the code. If you'd like to see it, <a href=https://github.com/charlievweiss/robot_learning>please click here.</a></p>
                            <p></p>
							<h2>Project Goal</h2>
                            <p>In this project, our goal was to track a person and always keep them in frame at a distance from one meter away.</p>
                            <p></p>
                            <h2>The Neural Net</h2>
                            <p>
                            To gather data, we placed the neato in a fixed location, while one of us walked in various paths in front of the neato to cover depth and horizontal location. The subject held an iPhone running an ARKit app that published its pose estimate. Meanwhile a script logged images from the neato as well as the phone’s position, which respectively constituted the training input and output for our neural network. As a first pass, we decided not to move the neato during data gathering in the hope of making the prediction work in this easier case and later improve the neural network. </p>
                            <p>Our network architecture includes a varying number of convolution/maxpooling pairs (convolution to extract features and maxpooling to reduce dimensionality), dropout layers (to prevent overfitting), and several decreasing dense layers to infer object position from the extracted features. Below is our most effective architecture. You’ll notice that it does not feature a dropout layer.</p>
                            <p><center><img src="../images/neuralnetparams.png" alt="" style="width: 75%"/></center></p>
                            <p></p>
                            <h2>Neato Implementation</h2>
                            <p><center><img src="../images/neato_neuralnet.png" alt="" style="width: 60%"/></center></p>
                            <p>The implementation on the Neato is very straightforward: The person_pos script obtains an image from the Neato, performs appropriate transforms to it (only resize currently), and feeds it through the neuralnet which outputs a Pose message The follower script obtains the Pose message, finds the distance and angle to the position, and applies proportional control to calculate a command velocity to center the person in frame at a distance 1m away. The command velocity is published to a node for the Neato.</p>
                            <p></p>
                            <h2>Design Decisions</h2>
                            <p>
                            The first important decision we made was to make the neural network predict subject position instead of commands to keep the subject in frame. This felt more useful as the robot could then do other things with that information (such as orbit around the subject). It also felt easier to get a good dataset, which leads us to our next decision.</p>
                            <p>We also decided to use ARKit as ground truth. This meant we didn’t have to deal with intermediate proxies for the subject position (such as position in the image or estimate from laser scan). It was fairly effective, though we did notice drift on the order of 15 cm after collecting data.</p>
                            <p>We also briefly considered modeling the subject’s velocity by comparing subsequent frames, but decided on making that a stretch goal.</p>
							<h2>Results</h2>
                            <p><center><img src="../images/neuralnet_results.png" alt="" style="width: 90%"/></center></p>
                            <p>At our best, the neural net predicted the person’s position in validation data set the quite well. Certain peaks aren’t well matched and the prediction is relatively noisy, but overall the correlation is clear. However, when we tested it in the same place wearing the same clothes … the neural net completely failed. We didn’t use the same Neato, so it’s possible there were differences in the camera quality and angle. We would expect that to perhaps produce offset or scaled results, but the predictions were significantly more flawed than that. It placed the subject further in images when the subject is close to the camera than when it is further away. This indicates that the model seriously overfit on both the training and validation dataset - which isn’t too crazy since they were taken from the same recording session. We tried adding dropout layers in the middle of the convolution, but that completely shot the neural net’s ability to make predictions.</p>
                            <h2>What would we do to improve our project if we had more time?</h2>
                            <p>There are several improvements we could make to this project. These include making further visualizations for analysis, data manipulation, training for different parameters, and noise filtering on the outputs.</p>
                                <h3>Visualizations</h3>
                                <p>Probably the most essential change is to add post-mortem visualizations to see what the neuralnet is valuing in the images we give it. As is, we don’t know what the net is looking at, so whatever hopeful changes we make to the data are based on our assumptions of what might be going wrong. It would be extremely helpful to know whether the net was looking at the person’s feet, legs, or something random in the surroundings. The only reason we don’t have this implemented were bizarre package distribution errors in necessary visualization add-ins, which we didn’t have time to fix before the due date.</p>
                                <h3>Data Manipulation</h3>
                                <p>We’ve hypothesized many possible changes to the data sets that might help train the neural net to be more accurate and flexible:</p>
                                <p>Check for drift in obtained positions Or rather, fix the drift we know is in the positions. ARkit is a great tool to get ground truth position measurements to go with the images, but it does drift over time. This means that the data set we used was inconsistent with its image and position matches, which could definitely have an effect on the accuracy of our model. With more time, this was a very obviously need fix. A first attempt would be to model the drift linearly eg. if y starts at 0.0 and ends at 0.10, then at the halfpoint add a -0.05 offset to the y. Shift images to black and white rather than colored The idea here is that we can train the net to function even when the person or environment has different colors than the training set (for example, if the person in the training images is wearing black pants, but the person in real life is wearing blue). This would reduce the layers of the image as well, giving the net fewer parameters to fit (or overfit) to. Train in different environments We obtained our dataset with all one environment and using the same person in the same outfit with the idea that it would be the simplest possible test case: The validation would be if it could follow the same person in the same place. However, this may have been misguided. If we took data from different environments, it could decrease the chance that the net’s taking bad clues from the background, because the person would be the consistency between different images. Change camera angle Besides the probability that the camera angle in person was different than that used while obtaining training images, it may have been better to angle it so that the entire person was visible. It would be interesting to see if this could work any better with more of a concept of “person” rather than “legs”, and is more in line with the original project goal of person tracking.</p>
                                <h3>Training for different parameters</h3> 
                                <p>There were two major modifications we could have made to the design of our neuralnet that could have helped with our project goal:</p>

                                <p>Train a second neuralnet to detect whether there is a person in the frame at all We realized partway in that our dataset had only examples of a person in the frame, so when there was no one there was no telling what it would predict. This is a pretty essential detection ability for our goal behavior, so it would be an important next step to implement (though it doesn’t necessarily have to be another neuralnet). <a href:"https://eng.uber.com/coordconv/">A blogpost from Uber</a> notes that standard CNN architectures fail to keep track of spatial information. Their fix is to add two channels, one that encodes the x pixel position, one that encodes the y pixel position. This allows a filter to access information about the pixel’s position in the image during a convolution.</p>
                                <h3>Noise filtering</h3>
                                <p>We got a lot of noise in the predicted positions from the neuralnet. This fluctuation could explain some of the false predictions in real time. The simple solution is to add something like a low pass filter to the prediction-- essentially, store the last 10 or so predictions and find some reasonable average between them. This would smooth out the noise in the predictions and maybe get a more accurate output.</p>
                                <h3>Investigation into Strange Outputs</h3>
                                <p>One of the models we generated predicted direction of movement, but at a fraction of the amplitude (if the person moved 1m to the left, it predicted 10cm to the left). This was the only model with a dropout layer (and a significant one at that-- it dropped half the data), which we thought would help with the overfitting problem. The actual results we got are honestly baffling, and worth looking into with some extra time.</p>
                        </div>
					</div>

				<!-- Footer 
					<footer id="footer">
						<div class="inner">
							<section>
								<h2>Get in touch</h2>
								<form method="post" action="#">
									<div class="field half first">
										<input type="text" name="name" id="name" placeholder="Name" />
									</div>
									<div class="field half">
										<input type="email" name="email" id="email" placeholder="Email" />
									</div>
									<div class="field">
										<textarea name="message" id="message" placeholder="Message"></textarea>
									</div>
									<ul class="actions">
										<li><input type="submit" value="Send" class="special" /></li>
									</ul>
								</form>
							</section>
							<section>
								<h2>Follow</h2>
								<ul class="icons">
									<li><a href="#" class="icon style2 fa-twitter"><span class="label">Twitter</span></a></li>
									<li><a href="#" class="icon style2 fa-facebook"><span class="label">Facebook</span></a></li>
									<li><a href="#" class="icon style2 fa-instagram"><span class="label">Instagram</span></a></li>
									<li><a href="#" class="icon style2 fa-dribbble"><span class="label">Dribbble</span></a></li>
									<li><a href="#" class="icon style2 fa-github"><span class="label">GitHub</span></a></li>
									<li><a href="#" class="icon style2 fa-500px"><span class="label">500px</span></a></li>
									<li><a href="#" class="icon style2 fa-phone"><span class="label">Phone</span></a></li>
									<li><a href="#" class="icon style2 fa-envelope-o"><span class="label">Email</span></a></li>
								</ul>
							</section>
							<ul class="copyright">
								<li>&copy; Untitled. All rights reserved</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</footer>-->

			</div>

		<!-- Scripts -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/skel.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../assets/js/main.js"></script>

	</body>
</html>